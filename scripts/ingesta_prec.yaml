# config.yaml
pipeline_name: datahub_source_prec
datahub_api:
  server: "http://datahub-gms-prec:8080"  # URL de la instancia de origen
  token: "eyJhbGciOiJIUzI1NiJ9.eyJhY3RvclR5cGUiOiJVU0VSIiwiYWN0b3JJZCI6ImRhdGFodWIiLCJ0eXBlIjoiUEVSU09OQUwiLCJ2ZXJzaW9uIjoiMiIsImp0aSI6IjMwZmU3NmFkLWY3ZmEtNDU5Ni05Mjc4LWM2NDU2NWNlYTIwYyIsInN1YiI6ImRhdGFodWIiLCJleHAiOjE3NDkxMjA0MjksImlzcyI6ImRhdGFodWItbWV0YWRhdGEtc2VydmljZSJ9.lLjC-TdBN45U4lwb7NAwPjEBlRq6aGrx79i76UGM53g" #lo podemos coger de NIfi o crear otro en datahub

source:
  type: "datahub"
  # Configuración de la fuente, para el acceso a la base de datos y Kafka
  config:
    database_connection:
      scheme: "mysql+pymysql"
      host_port: "mysql-prec:3306"
      database: "datahub"
      username: "datahub"
      password: "datahub"
    stateful_ingestion:
      enabled: true
      ignore_old_state: true
    urn_pattern:
      allow:
        - .*
      deny:
        - ^urn:li:dataJob.*

sink:
  # Configuración del destino para la ingesta
  type: "datahub-rest"
  config:
    server: "http://datahub-gms-federado:8080"  # URL de la instancia de destino